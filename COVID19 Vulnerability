import os
os.chdir('C:/Users/DELL/Documents/LMU classes/WS20/Machine Learning/Exam/Project/Data_rel/2019/')
#os.chdir('C:/Users/Flo/Desktop/ML/2019/')
print("current directory: ", os.getcwd())
import pandas as pd
import numpy as np
import os
import glob
import matplotlib.pyplot as plt

d = {}
for filename in glob.glob("*.csv"):
    d[os.path.basename(filename)] = pd.read_csv(os.path.basename(filename), error_bad_lines=False, encoding='cp1252')

res = list(d.keys())[0]
df = pd.DataFrame(data =list(d[str(res)]["ï»¿Country Name"]), columns= ["Country"])
for i in d.keys(): 
    df2 = pd.DataFrame(data = list(d[i]['2019']), columns = [d[i]['Indicator Name'][0]])
    df[[d[i]['Indicator Name'][0]]] = df2

df    

path = "C:/Users/DELL/Documents/LMU classes/WS20/Machine Learning/Exam/Project/Data_rel/"
#path = "C:/Users/Flo/Desktop/ML/"
os.chdir(path)
covid_data = pd.read_csv("covid19.csv")

df3 = pd.DataFrame(columns = ["Country", "Date", "Total Cases", "Total Deaths"])

old_country = covid_data.location[0]
for i, country in enumerate(covid_data.location):
    if old_country != country:
        df3 = df3.append({"Country": old_country, "Date": covid_data.date[i-1], "Total Cases": covid_data.total_cases_per_million[i-1], "Total Deaths": covid_data.total_deaths_per_million[i-1]}, ignore_index = True)
        old_country = country

df3

path = "C:/Users/DELL/Documents/LMU classes/WS20/Machine Learning/Exam/Project/Data_rel/2019"
#path = "C:/Users/Flo/Desktop/ML/2019"
os.chdir(path)
unemployment2020 = pd.read_csv("Unemployment.csv", error_bad_lines=False, encoding='cp1252')

df4 = pd.DataFrame({"Country": unemployment2020["Country Name"], unemployment2020["Indicator Name"][0]: unemployment2020["2020"]})
df4

path = "C:/Users/DELL/Documents/LMU classes/WS20/Machine Learning/Exam/Project/Data_rel/"
#path = "C:/Users/Flo/Desktop/ML/"
os.chdir(path)
debt = pd.read_csv("General gross debt.csv", error_bad_lines=False, encoding='cp1252')

df5 = pd.DataFrame(columns = ["Country", "General Gross Debt"])

indicator = debt.Indicator[0]
for i, indicator in enumerate(debt.Indicator):
    if indicator == "General government gross debt" and debt["Subindicator Type"][i] == "% of GDP":
        df5 = df5.append({"Country": debt["Country Name"][i], "General Gross Debt": debt["2019"][i]}, ignore_index = True)
        
df5

path = "C:/Users/DELL/Documents/LMU classes/WS20/Machine Learning/Exam/Project/Data_rel"
#path = "C:/Users/Flo/Desktop/ML"
os.chdir(path)
tourism = pd.read_csv("tourism.csv", error_bad_lines=False, encoding='cp1252')

df6 = pd.DataFrame({"Country": tourism["Country"], "Contribution of travel and tourism to GDP as a share of GDP(%)": tourism["2019"]})
df6

df_com = df.merge(df5,how='left', left_on='Country', right_on='Country')
df_com = df_com.merge(df6,how='left', left_on='Country', right_on='Country')
df_com = df_com.merge(df3,how='left', left_on='Country', right_on='Country')
df_com.drop(columns=['Date'])

df_com_pd = pd.DataFrame(df_com) 
df_com_pd.rename(columns = {'GDP growth (annual %)': 'GDP growth','Unemployment, total (% of total labor force) (modeled ILO estimate)':'unemployment','Foreign direct investment, net inflows (% of GDP)': 'fdi' ,'Food imports (% of merchandise imports)': 'food import', 'Fuel exports (% of merchandise exports)': 'fuel exports' ,'Net ODA received (% of GNI)': 'net ODA', 'Ores and metals exports (% of merchandise exports)': 'Ores and Metals', 'Personal remittances, received (% of GDP)':'remittances', 'Total reserves in months of imports':'reserves', 'General Gross Debt': 'Debt','Contribution of travel and tourism to GDP as a share of GDP(%)': 'T&T',}, inplace = True)
df_com_pd = df_com_pd[['Country','fdi', 'remittances', 'food import','fuel exports', 'net ODA', 'Ores and Metals', 'Debt', 'reserves', 'T&T', 'unemployment', 'GDP growth', 'Total Cases', 'Total Deaths']]
df_com_pd


from sklearn.linear_model import LinearRegression
import pandas as pd

df_drop = df_com_pd[['Country','fdi', 'remittances',  'unemployment', 'GDP growth',]]
df_drop = df_drop.dropna()
df_drop.reset_index(inplace= True, drop=True)

def pred_nan(var, df_drop, df_com_pd):
    df_alone = df_com_pd[['Country',var]]
    df_tofit1 = df_drop.merge(df_alone,how='left', left_on='Country', right_on='Country')

    df_tofit1 = df_tofit1[['fdi', 'remittances',  'unemployment', 'GDP growth', var]]
    index = df_tofit1.loc[pd.isna(df_tofit1[var]), :].index

    y = df_tofit1[df_tofit1[var].isnull()]
    df_tofit1_drop = df_tofit1.copy()
    df_tofit1_drop.dropna(inplace=True)

    y_train = df_tofit1_drop[var]
    X_train = df_tofit1_drop.drop(var, axis=1)
    X_test = y.drop(var, axis=1)

    model = LinearRegression()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    
    for c, i in enumerate(index):
        df_tofit1[var][i]=y_pred[c]
    
    df_drop[var] = df_tofit1[var]
        
    return
    
pred_nan('food import', df_drop, df_com_pd) 
pred_nan('fuel exports', df_drop, df_com_pd) 
pred_nan('net ODA', df_drop, df_com_pd) 
pred_nan('Ores and Metals', df_drop, df_com_pd) 
pred_nan('Debt', df_drop, df_com_pd) 
pred_nan('reserves', df_drop, df_com_pd) 
pred_nan('T&T', df_drop, df_com_pd) 
pred_nan('Total Cases', df_drop, df_com_pd)
pred_nan('Total Deaths', df_drop, df_com_pd)

df_drop = df_drop[['Country','fdi', 'remittances', 'food import','fuel exports', 'net ODA', 'Ores and Metals','Debt', 'reserves', 'T&T', 'unemployment', 'GDP growth', 'Total Cases', 'Total Deaths']]

df_drop

# OLS (BASELINE MODEL)

import statsmodels.api as sm

y = df_drop['unemployment']
X = df_drop.iloc[:, 1:9]

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()

#Check for linearity of the independent variables and the dependent variables

%matplotlib inline
%config InlineBackend.figure_format ='retina'
import seaborn as sns 
import matplotlib.pyplot as plt
import statsmodels.stats.api as sms
sns.set_style('darkgrid')
sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)

def linearity_test(model, y):
    '''
    Function for visually inspecting the assumption of linearity in a linear regression model.
    It plots observed vs. predicted values and residuals vs. predicted values.
    
    Args:
    * model - fitted OLS model from statsmodels
    * y - observed values
    '''
    fitted_vals = model.predict()
    resids = model.resid

    fig, ax = plt.subplots(1,2)
    
    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})
    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)
    ax[0].set(xlabel='Predicted', ylabel='Observed')

    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})
    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)
    ax[1].set(xlabel='Predicted', ylabel='Residuals')
    
linearity_test(lin_reg, y)    

#Checking for Expectation (mean) of residuals is zero
lin_reg.resid.mean()

#checking from multicollinearity

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame() 
vif_data["feature"] = X.columns 
  
# calculating VIF for each feature 
vif_data["VIF"] = [variance_inflation_factor(X.values, i) 
                          for i in range(len(X.columns))] 
  
print(vif_data)

Nothing is higher than 10. Good.

#Checking for Homoscedasticity, (equal variance) of residuals
#Breusch-Pagan and Goldfeld-Quandt. In both of them, the null hypothesis assumes homoscedasticity and a p-value below a 
#certain level (like 0.05) indicates we should reject the null in favor of heteroscedasticity.

%matplotlib inline
%config InlineBackend.figure_format ='retina'
import seaborn as sns 
import matplotlib.pyplot as plt
import statsmodels.stats.api as sms
sns.set_style('darkgrid')
sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)

def homoscedasticity_test(model):
    '''
    Function for testing the homoscedasticity of residuals in a linear regression model.
    It plots residuals and standardized residuals vs. fitted values and runs Breusch-Pagan and Goldfeld-Quandt tests.
    
    Args:
    * model - fitted OLS model from statsmodels
    '''
    fitted_vals = model.predict()
    resids = model.resid
    resids_standardized = model.get_influence().resid_studentized_internal

    fig, ax = plt.subplots(1,2)

    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})
    ax[0].set_title('Residuals vs Fitted', fontsize=16)
    ax[0].set(xlabel='Fitted Values', ylabel='Residuals')

    sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})
    ax[1].set_title('Scale-Location', fontsize=16)
    ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')

    bp_test = pd.DataFrame(sms.het_breuschpagan(resids, model.model.exog), 
                           columns=['value'],
                           index=['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value'])

    gq_test = pd.DataFrame(sms.het_goldfeldquandt(resids, model.model.exog)[:-1],
                           columns=['value'],
                           index=['F statistic', 'p-value'])

    print('\n Breusch-Pagan test ----')
    print(bp_test)
    print('\n Goldfeld-Quandt test ----')
    print(gq_test)
    print('\n Residuals plots ----')

homoscedasticity_test(lin_reg)

The results indicate that the assumption is not satisfied and we should reject the hypothesis of homoscedasticity.

df_drop['unemployment'].plot.hist()

df_drop['GDP growth'].plot.hist()

df_drop['Total Cases'].plot.hist()

df_drop['Total Deaths'].plot.hist()

#Transform dependent variables with log transformation

df_tofit = df_drop[['fdi','food import', 'fuel exports', 'net ODA', 'Ores and Metals', 'remittances', 'Debt','reserves', 'T&T',  'unemployment', 'GDP growth', 'Total Cases', 'Total Deaths']] 
df_ylog = df_tofit.copy()
df_alllog = df_tofit.copy()

for x in ['unemployment', 'GDP growth','Total Cases', 'Total Deaths']:
    df_ylog[x] = np.log(df_ylog[x])

data = df_ylog.copy() 
data.iloc[:,:9]

OLS, dependant variable = Unemployment

y = data['unemployment'] 
X = data.iloc[:, :9]

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()

X.drop(columns = ['fuel exports', 'fdi', 'remittances', 'reserves', 'T&T'], inplace= True)

X

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()

#Check for linearity of the independent variables and the dependent variables

%matplotlib inline
%config InlineBackend.figure_format ='retina'
import seaborn as sns 
import matplotlib.pyplot as plt
import statsmodels.stats.api as sms
sns.set_style('darkgrid')
sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)

def linearity_test(model, y):
    '''
    Function for visually inspecting the assumption of linearity in a linear regression model.
    It plots observed vs. predicted values and residuals vs. predicted values.
    
    Args:
    * model - fitted OLS model from statsmodels
    * y - observed values
    '''
    fitted_vals = model.predict()
    resids = model.resid

    fig, ax = plt.subplots(1,2)
    
    sns.regplot(x=fitted_vals, y=y, lowess=True, ax=ax[0], line_kws={'color': 'red'})
    ax[0].set_title('Observed vs. Predicted Values', fontsize=16)
    ax[0].set(xlabel='Predicted', ylabel='Observed')

    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[1], line_kws={'color': 'red'})
    ax[1].set_title('Residuals vs. Predicted Values', fontsize=16)
    ax[1].set(xlabel='Predicted', ylabel='Residuals')
    
linearity_test(lin_reg, y)    

#checking from multicollinearity

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_data = pd.DataFrame() 
vif_data["feature"] = X.columns 
  
# calculating VIF for each feature 
vif_data["VIF"] = [variance_inflation_factor(X.values, i) 
                          for i in range(len(X.columns))] 
  
print(vif_data)

%matplotlib inline
%config InlineBackend.figure_format ='retina'
import seaborn as sns 
import matplotlib.pyplot as plt
import statsmodels.stats.api as sms
sns.set_style('darkgrid')
sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)

def homoscedasticity_test(model):
    '''
    Function for testing the homoscedasticity of residuals in a linear regression model.
    It plots residuals and standardized residuals vs. fitted values and runs Breusch-Pagan and Goldfeld-Quandt tests.
    
    Args:
    * model - fitted OLS model from statsmodels
    '''
    fitted_vals = model.predict()
    resids = model.resid
    resids_standardized = model.get_influence().resid_studentized_internal

    fig, ax = plt.subplots(1,2)

    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})
    ax[0].set_title('Residuals vs Fitted', fontsize=16)
    ax[0].set(xlabel='Fitted Values', ylabel='Residuals')

    sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})
    ax[1].set_title('Scale-Location', fontsize=16)
    ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')

    bp_test = pd.DataFrame(sms.het_breuschpagan(resids, model.model.exog), 
                           columns=['value'],
                           index=['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value'])

    gq_test = pd.DataFrame(sms.het_goldfeldquandt(resids, model.model.exog)[:-1],
                           columns=['value'],
                           index=['F statistic', 'p-value'])

    print('\n Breusch-Pagan test ----')
    print(bp_test)
    print('\n Goldfeld-Quandt test ----')
    print(gq_test)
    print('\n Residuals plots ----')

homoscedasticity_test(lin_reg)

from sklearn.metrics import mean_squared_error 
from sklearn.metrics import r2_score

regr = LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1) 
regr.fit(X, y)

# Make in-sample predictions 
y_pred = regr.predict(X)
# The mean squared error
print('Mean squared error in-sample: %.3f'
      % mean_squared_error(y, y_pred))
# In-sample R-sq.
print('In-sample Rsq: %.4f'
      % r2_score(y, y_pred))
print('Coefficients: \n', regr.coef_)

## Use K-fold out-of-sample validation to compare the predictive powers of the models
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import KFold, cross_val_score

X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('regr', LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1))])
ols_score_unemp = cross_val_score(pipe, X, y, cv=kf)
print(ols_score_unemp)
print("Accuracy: %0.5f (+/- %0.5f)" % (ols_score_unemp.mean(), ols_score_unemp.std() * 2))

OLS, dependant variable = Total Cases

#Drop the NaN in Total Cases and Total Deaths that came to be because there might countries who have 0 cases. And when we take log(0) = infinity (NaN)
data_drop_case = data.copy()
data_drop_case = data_drop_case.dropna(subset = ['Total Cases'])
y = data_drop_case['Total Cases'] 
X = data_drop_case.iloc[:, :9]

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()

X.drop(columns = ['fdi', 'fuel exports', 'Ores and Metals', 'Debt', 'reserves'], inplace= True)

X

regr = LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1) 
regr.fit(X, y)

# Make in-sample predictions 
y_pred = regr.predict(X)
# The mean squared error
print('Mean squared error in-sample: %.3f'
      % mean_squared_error(y, y_pred))
# In-sample R-sq.
print('In-sample Rsq: %.4f'
      % r2_score(y, y_pred))
print('Coefficients: \n', regr.coef_)

X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('regr', LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1))])

ols_score_cases = cross_val_score(pipe, X, y, cv=kf)
print(ols_score_cases)
print("Accuracy: %0.5f (+/- %0.5f)" % (ols_score_cases.mean(), ols_score_cases.std() * 2))

OLS, dependant variable = Total Deaths

data_drop_deaths = data.copy()
data_drop_deaths.dropna(subset = ["Total Deaths"], inplace = True)
y = data_drop_deaths['Total Deaths'] 
X = data_drop_deaths.iloc[:, :9]

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()

X.drop(columns = ['fuel exports', 'Ores and Metals', 'Debt', 'reserves', 'T&T'], inplace= True)

X

regr = LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1) 
regr.fit(X, y)

# Make in-sample predictions 
y_pred = regr.predict(X)
# The mean squared error
print('Mean squared error in-sample: %.3f'
      % mean_squared_error(y, y_pred))
# In-sample R-sq.
print('In-sample Rsq: %.4f'
      % r2_score(y, y_pred))
print('Coefficients: \n', regr.coef_)

X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('regr', LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1))])
ols_score_deaths = cross_val_score(pipe, X, y, cv=kf)
print(ols_score_deaths)
print("Accuracy: %0.5f (+/- %0.5f)" % (ols_score_deaths.mean(), ols_score_deaths.std() * 2))

OLS, dependent variable = GDP growth

data_drop_gdp = data.copy()
data_drop_gdp.dropna(subset=['GDP growth'],inplace= True)
y = data_drop_gdp['GDP growth'] 
X = data_drop_gdp.iloc[:, :9]

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()


#fuel exports has p-value > 0.05
X.drop(columns = ['fdi','food import', 'net ODA', 'Ores and Metals', 'reserves', 'T&T'], inplace= True)

X

X_constant = sm.add_constant(X)
lin_reg = sm.OLS(y,X_constant).fit()
lin_reg.summary()

regr = LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1) 
regr.fit(X, y)

# Make in-sample predictions 
y_pred = regr.predict(X)
# The mean squared error
print('Mean squared error in-sample: %.3f'
      % mean_squared_error(y, y_pred))
# In-sample R-sq.
print('In-sample Rsq: %.4f'
      % r2_score(y, y_pred))
print('Coefficients: \n', regr.coef_)


X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('regr', LinearRegression(fit_intercept = True, normalize=False, copy_X=False, n_jobs=-1))])
ols_score_gdp = cross_val_score(pipe, X, y, cv=kf)
print(ols_score_gdp)
print("Accuracy: %0.5f (+/- %0.5f)" % (ols_score_gdp.mean(), ols_score_gdp.std() * 2))

fig, ax = plt.subplots()
ax.boxplot([ols_score_unemp, ols_score_gdp, ols_score_cases, ols_score_deaths])
plt.xticks(range(1,6), ['Unemployment', 'GDP', 'Total Cases', 'Total Deaths'])
plt.ylabel('R squared')
plt.title('OLS: Out-of-sample validation')
plt.show()

Conclusion for OLS: Unemployment and log unemployment seemed to not be a good predictor variable for our inputs. log GDP is better

# RANDOM FOREST (Bagging of CART trees)

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_validate

from sklearn import ensemble
from sklearn import tree as tree

Random Forest, dependent variable = log Unemployment 

#The optimal number of trees haven's been identified from this step yet. Change n_estimators to your preferred number of trees
data = df_ylog.copy() 
y = data['unemployment'] 
X = data.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 

rf = ensemble.RandomForestRegressor(n_estimators = 164, min_samples_leaf = 3, min_samples_split = 2, oob_score = True, \
        random_state = 1)
rf = rf.fit(X_train, y_train)

train_pred = rf.predict(X_train)
y_pred = rf.predict(X_test)

# Out-of-sample R-sq.
print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

plt.plot(train_pred,"*", c = 'DarkBlue')
plt.plot(y_train, "*", c = 'orange')

#extra randomization (for each individual tree, we randomly select a subsample of covariates from X to split on)

rf = ensemble.RandomForestRegressor(n_estimators = 164, min_samples_leaf = 3, oob_score = True, \
        random_state = 1, max_features = int(X_train.shape[1]/3))
rf = rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
# Out-of-sample R-sq.
print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

The model performs better with randomization

#Use cross-validation to determine the optimal number of trees
#Another hyperparameter that we can tune is min_samples_leaf which means the minimum number of samples required to be at a leaf node
#min_samples_leaf = 3 is what was suggested in the tutorial. However, I am thinking of using a method I found online to find the best one
#Will update this later
def cv_iter(n, folds, X, y):
    kf = KFold(n_splits=folds, shuffle=True, random_state=1)
    rf = ensemble.RandomForestRegressor(n_estimators = n, min_samples_leaf = 3, \
        random_state = 1)
    cv_result = cross_validate(rf, X, y, scoring = 'neg_mean_squared_error')
    avg_score = np.mean(cv_result['test_score'])
    return float(-avg_score)

y = data['unemployment'] 
X = data.iloc[:, :9]

result = {}
for i in range(1, 200, 1):
    result[i] = cv_iter(i, 5, X, y)

matplotlib.rcParams.update({'font.size': 8, 'figure.figsize': (4, 3), 'figure.dpi': 100})    
results = pd.DataFrame.from_dict(result,orient='index', columns = ['Test Score'])
results.plot(color = 'DarkBlue')
tree_unemp = results['Test Score'].argmin(axis = 0)

print("Number of trees with the lowest MSE is: " + str(tree_unemp))

#My attempt to use cross-validation to choose the combination of number of trees and min_sample_leaf(minimum number of samples required to be at a leaf node)
#But it failed
import time
start = time.process_time()  

y = data['unemployment'] 
X = data.iloc[:, :9]

result = np.ones((200, 5))
for n in range(1, 201, 1):
    for m in range (1, 6, 1):
      result[n-1,m-1] = cv_iter(n, m, 5, X, y)

cross_val_unemp = np.unravel_index(result.argmin(), result.shape)
tree_unemp = cross_val_unemp[0]+1
leaf_unemp = cross_val_unemp[1]+1

print("Number of trees with the lowest MSE is: ", tree_unemp)
print("Number of leaf with the lowest MSE is: ", leaf_unemp)
print(time.process_time() - start)

Random forests, dependent variable = GDP growth

#The optimal number of trees haven's been identified from this step yet. Change n_estimators to your preferred number of trees
y = data_drop_gdp['GDP growth'] 
X = data_drop_gdp.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 

rf = ensemble.RandomForestRegressor(n_estimators = 112, min_samples_leaf = 3, min_samples_split = 2, oob_score = True, \
        random_state = 1)
rf = rf.fit(X_train, y_train)

train_pred = rf.predict(X_train)

y_pred = rf.predict(X_test)
# Out-of-sample R-sq.
print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

#extra randomization
rf = ensemble.RandomForestRegressor(n_estimators = 112, min_samples_leaf = 3, oob_score = True, \
        random_state = 1, max_features = int(X_train.shape[1]/3))
rf = rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
# Out-of-sample R-sq.
print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

The model performs better with randomization

#Use cross-validation to determine the optimal number of trees
y = data_drop_gdp['GDP growth'] 
X = data_drop_gdp.iloc[:, :9]

result = {}
for i in range(1, 200, 1):
    result[i] = cv_iter(i, 5, X, y)

matplotlib.rcParams.update({'font.size': 8, 'figure.figsize': (4, 3), 'figure.dpi': 100})    
results = pd.DataFrame.from_dict(result,orient='index', columns = ['Test Score'])
results.plot(color = 'DarkBlue')

tree_gdp = results['Test Score'].argmin(axis = 0)

print("Number of trees with the lowest MSE is: " + str(tree_gdp))

plt.plot(train_pred,"*", c = 'DarkBlue')
plt.plot(y_train, "*", c = 'orange')

Random Forest, dependent variable = Total cases

#The optimal number of trees haven's been identified from this step yet. Change n_estimators to your preferred number of trees
y = data_drop_case['Total Cases'] 
X = data_drop_case.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 

rf = ensemble.RandomForestRegressor(n_estimators = 156, min_samples_leaf = 3, min_samples_split = 2, oob_score = True, \
        random_state = 1)
rf = rf.fit(X_train, y_train)

train_pred = rf.predict(X_train)
y_pred = rf.predict(X_test)
# Out-of-sample R-sq.
print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

#extra randomization
rf = ensemble.RandomForestRegressor(n_estimators = 156, min_samples_leaf = 3, oob_score = True, \
        random_state = 1, max_features = int(X_train.shape[1]/3))
rf = rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

The model performs worse with randomization

#Use cross-validation to determine the optimal number of trees
#stopping rule of min_samples_leaf = 3.
y = data_drop_case['Total Cases'] 
X = data_drop_case.iloc[:, :9]

result = {}
for i in range(1, 200, 1):
    result[i] = cv_iter(i, 5, X, y)

matplotlib.rcParams.update({'font.size': 8, 'figure.figsize': (4, 3), 'figure.dpi': 100})    
results = pd.DataFrame.from_dict(result,orient='index', columns = ['Test Score'])
results.plot(color = 'DarkBlue')

tree_cases = results['Test Score'].argmin(axis = 0)

print("Number of trees with the lowest MSE is: " + str(tree_cases))

plt.plot(train_pred,"*", c = 'DarkBlue')
plt.plot(y_train, "*", c = 'orange')

Random Forest, dependent variable = Total deaths

#The optimal number of trees haven's been identified from this step yet. Change n_estimators to your preferred number of trees
y = data_drop_deaths['Total Deaths'] 
X = data_drop_deaths.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) 

rf = ensemble.RandomForestRegressor(n_estimators = 26, min_samples_leaf = 3, min_samples_split = 2, oob_score = True, \
        random_state = 1)
rf = rf.fit(X_train, y_train)

train_pred = rf.predict(X_train)
y_pred = rf.predict(X_test)

print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

#extra randomization
rf = ensemble.RandomForestRegressor(n_estimators = 26, min_samples_leaf = 3, oob_score = True, \
        random_state = 1, max_features = int(X_train.shape[1]/3))
rf = rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
# Out-of-sample R-sq.
print('Out-of-sample Rsq: %.3f'
      % r2_score(y_test, y_pred))
print('Out-of-sample MSE: %.3f'
      % mean_squared_error(y_test, y_pred))

The model performs better with randomization

#Use cross-validation to determine the optimal number of trees
#stopping rule of min_samples_leaf = 3.
y = data_drop_deaths['Total Deaths'] 
X = data_drop_deaths.iloc[:, :9]

result = {}
for i in range(1, 200, 1):
    result[i] = cv_iter(i, 5, X, y)

matplotlib.rcParams.update({'font.size': 8, 'figure.figsize': (4, 3), 'figure.dpi': 100})    
results = pd.DataFrame.from_dict(result,orient='index', columns = ['Test Score'])
results.plot(color = 'DarkBlue')

tree_deaths = results['Test Score'].argmin(axis = 0)

print("Number of trees with the lowest MSE is: " + str(tree_deaths))

plt.plot(train_pred,"*", c = 'DarkBlue')
plt.plot(y_train, "*", c = 'orange')

Use K-fold out-of-sample validation to compare the predictive power of OLS and Random Forest (Results for Random Forests)

from sklearn.ensemble import RandomForestRegressor

y = data['unemployment']
X = data.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ("forest", RandomForestRegressor(random_state=0,
                                                       n_estimators=tree_unemp))])
rf_score_unemp = cross_val_score(pipe, X, y, cv=kf)
print(rf_score_unemp)
print("Accuracy: %0.5f (+/- %0.5f)" % (rf_score_unemp.mean(), rf_score_unemp.std() * 2))

y = data_drop_gdp['GDP growth']
X = data_drop_gdp.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ("forest", RandomForestRegressor(random_state=0, n_estimators=tree_gdp))])

rf_score_gdp = cross_val_score(pipe, X, y, cv=kf)
print(rf_score_gdp)
print("Accuracy: %0.5f (+/- %0.5f)" % (rf_score_gdp.mean(), rf_score_gdp.std() * 2))

y = data_drop_case['Total Cases']
X = data_drop_case.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ("forest", RandomForestRegressor(random_state=0, n_estimators=tree_cases))])

rf_score_cases = cross_val_score(pipe, X, y, cv=kf)
print(rf_score_cases)
print("Accuracy: %0.5f (+/- %0.5f)" % (rf_score_cases.mean(), rf_score_cases.std() * 2))

y = data_drop_deaths['Total Deaths']
X = data_drop_deaths.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ("forest", RandomForestRegressor(random_state=0, n_estimators=tree_deaths))])

rf_score_deaths = cross_val_score(pipe, X, y, cv=kf)
print(rf_score_deaths)
print("Accuracy: %0.5f (+/- %0.5f)" % (rf_score_deaths.mean(), rf_score_deaths.std() * 2))

fig, ax = plt.subplots()
ax.boxplot([rf_score_unemp, rf_score_gdp, rf_score_cases, rf_score_deaths])
plt.xticks(range(1,5), ['Unemployment', 'GDP', 'Total Cases', 'Total Deaths'])
plt.ylabel('R squared')
plt.title('Random Forest: Out-of-sample validation')
plt.show()

# MODEL COMPARISON: OLS V.S RANDOM FOREST


fig, ax = plt.subplots()
ax.boxplot([ols_score_unemp, rf_score_unemp])
plt.xticks(range(1,3), ['OLS', 'Random Forest'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for Unemployment')
plt.show()

fig, ax = plt.subplots()
ax.boxplot([ols_score_gdp, rf_score_gdp])
plt.xticks(range(1,3), ['OLS', 'Random Forest'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for GDP growth')
plt.show()

fig, ax = plt.subplots()
ax.boxplot([ols_score_cases, rf_score_cases])
plt.xticks(range(1,3), ['OLS', 'Random Forest'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for Total Cases')
plt.show()

fig, ax = plt.subplots()
ax.boxplot([ols_score_deaths, rf_score_deaths])
plt.xticks(range(1,3), ['OLS', 'Random Forest'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for Total Deaths')
plt.show()

# LASSO

LASSO, dependent variable = log Unemployment

from sklearn.linear_model import Lasso
data = df_ylog.copy() 
y = data['unemployment'] 
X = data.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', 
                  Lasso(alpha=0.05, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])
# The alpha here is equal to what is often called lambda in explanations of the theory of LASSO.
# Here we just take some value for alpha, it is not the optimal one.

pipe.fit(X_train, y_train)
print('In-sample Rsq: % .4f'
     % pipe.score(X_train, y_train))
print('Out-of-sample Rsq: % .4f'
     % pipe.score(X_test, y_test))


The model performs worse than OLS.

#determine regularization path, i.e. try out different values for alpha and see how
#the estimated coefficients of our model behave. This can give us an idea for the candidate models.

from sklearn.linear_model import lasso_path

y = data['unemployment'] 
X = data.iloc[:, :9]
X = StandardScaler().fit_transform(X) 

# Compute paths
epsilon = 0.0015 #This is the starting value of our alpha. The smaller it is the longer is the path. 0 = OLS.
alphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps=epsilon, max_iter=10000, fit_intercept=True)

# Display results
plt.figure()
log_alphas_lasso = np.log10(alphas_lasso)
for coef_l in coefs_lasso:
    l1 = plt.plot(log_alphas_lasso, coef_l)

plt.xlabel('Log10(alpha)')
plt.ylabel('coefficients')
plt.title('Lasso Path')
plt.axis('tight')
plt.show()


# Select optimal alpha with cross-validation and information criteria.
# Use K-fold out-of-sample validation to select the 'best' alpha.

from sklearn.linear_model import LassoCV
import time

y = data['unemployment'] 
X = data.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lassocv', LassoCV(cv=5, max_iter=10000, alphas=np.logspace(-3, -0.5, 100), fit_intercept=True))]) #Here we use 5 iterations, take 100 alphas in the range from -3 to -0.5, estimate these 100 models 5 times, i.e. estimate 500 models.
t1 = time.time()
pipe.fit(X, y)
t_cv = time.time() - t1

l = pipe.named_steps.lassocv #the output of lassocv step is stored here
print("Optimal alpha = % .4f " % l.alpha_)
print("Training time = % .4fs " % t_cv)

# Display results
log_alphas_lasso = np.log10(l.alphas_)

plt.figure()
plt.plot(log_alphas_lasso , l.mse_path_, ':')
plt.plot(log_alphas_lasso , l.mse_path_.mean(axis=-1), 'k',
         label='Average across the folds', linewidth=2)
plt.axvline(np.log10(l.alpha_) , linestyle='--', color='k',
            label='alpha: CV estimate')

plt.legend()

plt.xlabel(r'log10 $\alpha$')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: coordinate descent ')
plt.axis('tight')

Use K-fold out-of-sample validation to compare the predictive power of OLS and LASSO

y = data['unemployment'] 
X = data.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', Lasso(alpha=l.alpha_, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])
lasso_score_unemp = cross_val_score(pipe, X, y, cv=kf)
print(lasso_score_unemp)
print("Accuracy: %0.5f (+/- %0.5f)" % (lasso_score_unemp.mean(), lasso_score_unemp.std() * 2))


LASSO, dependent variable = GDP growth

y = data_drop_gdp['GDP growth'] 
X = data_drop_gdp.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', 
                  Lasso(alpha=0.05, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])

pipe.fit(X_train, y_train)
print('In-sample Rsq: % .4f'
     % pipe.score(X_train, y_train))
print('Out-of-sample Rsq: % .4f'
     % pipe.score(X_test, y_test))

# Select optimal alpha with cross-validation and information criteria.
# Use K-fold out-of-sample validation to select the 'best' alpha.

y = data_drop_gdp['GDP growth'] 
X = data_drop_gdp.iloc[:, :9]

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lassocv', LassoCV(cv=5, max_iter=10000, alphas=np.logspace(-3, -0.5, 100), fit_intercept=True))]) #Here we use 5 iterations, take 100 alphas in the range from -3 to -0.5, estimate these 100 models 5 times, i.e. estimate 500 models.
t1 = time.time()
pipe.fit(X, y)
t_cv = time.time() - t1

l = pipe.named_steps.lassocv #the output of lassocv step is stored here
print("Optimal alpha = % .4f " % l.alpha_)
print("Training time = % .4fs " % t_cv)

# Display results
log_alphas_lasso = np.log10(l.alphas_)

plt.figure()
plt.plot(log_alphas_lasso , l.mse_path_, ':')
plt.plot(log_alphas_lasso , l.mse_path_.mean(axis=-1), 'k',
         label='Average across the folds', linewidth=2)
plt.axvline(np.log10(l.alpha_) , linestyle='--', color='k',
            label='alpha: CV estimate')

plt.legend()

plt.xlabel(r'log10 $\alpha$')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: coordinate descent ')
plt.axis('tight')

Use K-fold out-of-sample validation to compare the predictive power of OLS and LASSO

y = data_drop_gdp['GDP growth'] 
X = data_drop_gdp.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', Lasso(alpha=l.alpha_, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])
lasso_score_gdp = cross_val_score(pipe, X, y, cv=kf)
print(lasso_score_gdp)
print("Accuracy: %0.5f (+/- %0.5f)" % (lasso_score_gdp.mean(), lasso_score_gdp.std() * 2))

LASSO, dependent variable = Total cases

y = data_drop_case['Total Cases'] 
X = data_drop_case.iloc[:, :9]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', 
                  Lasso(alpha=0.05, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])

pipe.fit(X_train, y_train)
print('In-sample Rsq: % .4f'
     % pipe.score(X_train, y_train))
print('Out-of-sample Rsq: % .4f'
     % pipe.score(X_test, y_test))

# Select optimal alpha with cross-validation and information criteria.
# Use K-fold out-of-sample validation to select the 'best' alpha.

y = data_drop_case['Total Cases'] 
X = data_drop_case.iloc[:, :9]

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lassocv', LassoCV(cv=5, max_iter=10000, alphas=np.logspace(-3, -0.5, 100), fit_intercept=True))]) #Here we use 5 iterations, take 100 alphas in the range from -3 to -0.5, estimate these 100 models 5 times, i.e. estimate 500 models.
t1 = time.time()
pipe.fit(X, y)
t_cv = time.time() - t1

l = pipe.named_steps.lassocv #the output of lassocv step is stored here
print("Optimal alpha = % .4f " % l.alpha_)
print("Training time = % .4fs " % t_cv)

# Display results
log_alphas_lasso = np.log10(l.alphas_)

plt.figure()
plt.plot(log_alphas_lasso , l.mse_path_, ':')
plt.plot(log_alphas_lasso , l.mse_path_.mean(axis=-1), 'k',
         label='Average across the folds', linewidth=2)
plt.axvline(np.log10(l.alpha_) , linestyle='--', color='k',
            label='alpha: CV estimate')

plt.legend()

plt.xlabel(r'log10 $\alpha$')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: coordinate descent ')
plt.axis('tight')

Use K-fold out-of-sample validation to compare the predictive power of OLS and LASSO

y = data_drop_case['Total Cases'] 
X = data_drop_case.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', Lasso(alpha=l.alpha_, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])
lasso_score_cases = cross_val_score(pipe, X, y, cv=kf)
print(lasso_score_cases)
print("Accuracy: %0.5f (+/- %0.5f)" % (lasso_score_cases.mean(), lasso_score_cases.std() * 2))

LASSO, dependent variable = Total deaths

y = data_drop_deaths['Total Deaths'] 
X = data_drop_deaths.iloc[:, :9]


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', 
                  Lasso(alpha=0.05, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])

pipe.fit(X_train, y_train)
print('In-sample Rsq: % .4f'
     % pipe.score(X_train, y_train))
print('Out-of-sample Rsq: % .4f'
     % pipe.score(X_test, y_test))

# Select optimal alpha with cross-validation and information criteria.
# Use K-fold out-of-sample validation to select the 'best' alpha.

from sklearn.linear_model import LassoCV
import time

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lassocv', LassoCV(cv=5, max_iter=10000, alphas=np.logspace(-3, -0.5, 100), fit_intercept=True))]) #Here we use 5 iterations, take 100 alphas in the range from -3 to -0.5, estimate these 100 models 5 times, i.e. estimate 500 models.
t1 = time.time()
pipe.fit(X, y)
t_cv = time.time() - t1

l = pipe.named_steps.lassocv #the output of lassocv step is stored here
print("Optimal alpha = % .4f " % l.alpha_)
print("Training time = % .4fs " % t_cv)

# Display results
log_alphas_lasso = np.log10(l.alphas_)

plt.figure()
plt.plot(log_alphas_lasso , l.mse_path_, ':')
plt.plot(log_alphas_lasso , l.mse_path_.mean(axis=-1), 'k',
         label='Average across the folds', linewidth=2)
plt.axvline(np.log10(l.alpha_) , linestyle='--', color='k',
            label='alpha: CV estimate')

plt.legend()

plt.xlabel(r'log10 $\alpha$')
plt.ylabel('Mean square error')
plt.title('Mean square error on each fold: coordinate descent ')
plt.axis('tight')

Use K-fold out-of-sample validation to compare the predictive power of OLS and LASSO

y = data_drop_deaths['Total Deaths'] 
X = data_drop_deaths.iloc[:, :9]
X = pd.DataFrame(np.ascontiguousarray(X.values), columns=X.columns)

kf = KFold(n_splits=5, shuffle=True, random_state=1)

pipe = Pipeline([('scaler', StandardScaler()),
                 ('lasso', Lasso(alpha=l.alpha_, fit_intercept=True, normalize=False, copy_X=False, max_iter=10000))])
lasso_score_deaths = cross_val_score(pipe, X, y, cv=kf)
print(lasso_score_deaths)
print("Accuracy: %0.5f (+/- %0.5f)" % (lasso_score_deaths.mean(), lasso_score_deaths.std() * 2))

# MODEL COMPARISON: OLS VS. RANDOM FOREST VS. LASSO

fig, ax = plt.subplots()
ax.boxplot([ols_score_unemp, rf_score_unemp, lasso_score_unemp])
plt.xticks(range(1,4), ['OLS', 'Random Forest', 'LASSO'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for Unemployment')
plt.show()

fig, ax = plt.subplots()
ax.boxplot([ols_score_gdp, rf_score_gdp, lasso_score_gdp])
plt.xticks(range(1,4), ['OLS', 'Random Forest', 'LASSO'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for GDP growth')
plt.show()

fig, ax = plt.subplots()
ax.boxplot([ols_score_cases, rf_score_cases, lasso_score_cases])
plt.xticks(range(1,4), ['OLS', 'Random Forest', 'LASSO'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for Total Cases')
plt.show()

fig, ax = plt.subplots()
ax.boxplot([ols_score_deaths, rf_score_deaths, lasso_score_deaths])
plt.xticks(range(1,4), ['OLS', 'Random Forest', 'LASSO'])
plt.ylabel('R squared')
plt.title('Out-of-sample validation for Total Deaths')
plt.show()

df_sorted_unemp = df_com_pd.sort_values(by= 'unemployment', ascending=False, inplace=False, na_position='last', ignore_index=False)
print("The 15 most vulnerable countries according to Unemployment rate in 2019 \n")
print(df_sorted_unemp.head(15).iloc[:,[0, 10]])

df_sorted_gdp = df_com_pd.sort_values(by= 'GDP growth', ascending=True, inplace=False, na_position='last', ignore_index=False)
print("The 15 most vulnerable countries according to the GDP growth in 2019 \n")
print(df_sorted_gdp.head(15).iloc[:,[0, 11]])

df3_sorted_cases = df3.sort_values(by= 'Total Cases', ascending=False, inplace=False, na_position='last', ignore_index=False)
print("The 15 most vulnerable countries according to total COVID-19 cases as of 27 February 2021\n")
print(df3_sorted_cases.head(15).iloc[:,[0, 2]])



df3_sorted_deaths = df3.sort_values(by= 'Total Deaths', ascending=False, inplace=False, na_position='last', ignore_index=False)
print("The 15 most vulnerable countries according to total deaths from COVID-19 as of 27 February 2021\n")
print(df3_sorted_deaths.head(15).iloc[:,[0, 3]])
